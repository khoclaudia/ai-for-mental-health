# -*- coding: utf-8 -*-
"""AI Project Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lulcAelY-1IfA_ZTgqu-9zF5cbGHo1JT
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# % cd drive/My\ Drive/AI\ for\ Social\ Good\ Spring\ 20/

# Commented out IPython magic to ensure Python compatibility.
# % pwd

# import numpy as np
# import pandas as pd
# import csv
# import torch
# import sys
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim

# from torch.autograd import Variable
# from torch.utils import data
# from torchvision import transforms, datasets, models

# def split(path, K):
#     parsedData = np.array(pd.read_csv(path)) # Shape: (1259, 24)
#     splitData = [[] for i in range(K)]
    
#     # Use k = 10 ==> 9 sets of 126 elements, 1 set of 125 elements
#     for idx in np.random.permutation(parsedData.shape[0]):
#         splitData[idx%K].append(parsedData[idx])

#     return [np.array(splitData[i]) for i in range(K)]

# K = 10
# splitData = split('clean_data.csv', K)
# for iter in range(K):
#     testData = splitData[iter]
#     trainData = np.concatenate(splitData[:iter]+splitData[iter+1:], axis=0)

#     # TARGET COLUMN IS INDEX 4

# class Baseline(nn.Module):
#     def __init__(self, size_list):
#         super(Baseline, self).__init__()
#         layers = []
#         self.size_list = size_list
#         for i in range(len(size_list) - 2):
#             layers.append(nn.Linear(size_list[i],size_list[i+1]))
#             layers.append(nn.ReLU())
#             layers.append(nn.BatchNorm1d(size_list[i+1]))
#             layers.append(nn.Dropout(p=0.15))
#         layers.append(nn.Linear(size_list[-2], size_list[-1]))
#         self.net = nn.Sequential(*layers)
#         self.net = self.net.float()

#     def forward(self, x):
#         return self.net(x.float())

# def init_xavier(m):
#     if type(m) == nn.Linear:
#         fan_in = m.weight.size()[1]
#         fan_out = m.weight.size()[0]
#         std = np.sqrt(2.0 / (fan_in + fan_out))
#         m.weight.data.normal_(0,std)

# model = Baseline([24, 1024, 1024, 1024, 512, 512, 1])
# model.apply(init_xavier)
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(model.parameters(), lr=0.01)
# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import os
import csv
import numpy as np
import torch
import torchvision
import sys
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import math
from torch.autograd import Variable
from torch.utils import data
from torchvision import transforms, datasets, models
from collections import namedtuple
import matplotlib.pyplot as plt
import time
import pandas as pd
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
# from kfold import split

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

def split(path, K):
    parsedData = np.array(pd.read_csv(path)) # Shape: (1259, 24)
    splitData = [[] for i in range(K)]
    
    # Use k = 10 ==> 9 sets of 126 elements, 1 set of 125 elements
    for idx in np.random.permutation(parsedData.shape[0]):
        splitData[idx%K].append(parsedData[idx])

    splitData = [np.array(splitData[i]) for i in range(K)]
    
    # Swaps the 4th column with the last column, so the target is at the end
    for i in range(K):
        splitData[i][:,[4, -1]] = splitData[i][:,[-1, 4]]
    return splitData

class Dataset(data.Dataset):
    def __init__(self, dataset):
        # assumes the target is in the last column
        self.data = dataset[:,:-1]
        self.target = dataset[:,-1]
        self.n_class = 2
        
    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return torch.from_numpy(self.data[index]), self.target[index]

K = 10
batchsize = 64

split_data = split('clean_data.csv', K)

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_sizes, output_dim):
        super(MLP, self).__init__()
        layers = []
        self.hidden_sizes = hidden_sizes        
        for i in range(len(hidden_sizes)):
            if (i == 0):
                layers.append(nn.Linear(input_dim,hidden_sizes[i]))
            else:
                layers.append(nn.Linear(hidden_sizes[i-1],hidden_sizes[i]))
            layers.append(nn.ReLU())
            layers.append(nn.BatchNorm1d(hidden_sizes[i]))
            layers.append(nn.Dropout(p=0.45))
        layers.append(nn.Linear(hidden_sizes[-1], output_dim))
        self.layers = nn.Sequential(*layers).float()

    def forward(self, x):
        return self.layers(x.float())
        

def init_xavier(m):
    if (type(m) == nn.Linear):
        fan_in = m.weight.size()[1]
        fan_out = m.weight.size()[0]
        std = np.sqrt(2.0 / (fan_in + fan_out))
        m.weight.data.normal_(0,std)

def train_single_epoch(model, data_loader):
    model.train()
    # model.to(device)
    for batch_num, (feats, labels) in enumerate(data_loader):
        feats, labels = feats.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(feats)
        loss = criterion(outputs, labels.long())
        loss.backward()
        optimizer.step()
        
        torch.cuda.empty_cache()
        del feats
        del labels
        del loss

def test_classify(model, loader):
    model.eval()
    test_loss = []
    accuracy = 0
    total = 0

    for batch_num, (feats, labels) in enumerate(loader):
        
        feats, labels = feats.to(device), labels.to(device)
        outputs = model(feats)
        
        # print("OUTPUTS:\n")
        # print(outputs.shape)
        # print(outputs)
        # print("END")

        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)
        # pred_labels = pred_labels.view(-1)
        
        loss = criterion(outputs, labels.long())
        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()
        total += len(labels)
        test_loss.extend([loss.item()]*feats.size()[0])
        del feats
        del labels
        # torch.cuda.empty_cache()

    model.train()
    return np.mean(test_loss), accuracy/total

# Commented out IPython magic to ensure Python compatibility.
# % ls -als

# print(device)
# a = torch.rand(10,10)
# a = a.to(device)

# Training the model
train_accuracies = {}
test_accuracies = {}
train_losses = {}
test_losses = {}


for iter in range(K):
# for iter in range(K):
    avg_train_loss, avg_train_acc = 0, 0
    avg_test_loss, avg_test_acc = 0, 0

    # Initializing the model and other parameters
    model = MLP(23, [512,1024,1024,2048,4096,1024,256], 2)
    model.to(device)
    model.apply(init_xavier)
    num_epochs = 150
    learning_rate = 5e-4
    weightdecay = 5e-3
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    criterion = nn.CrossEntropyLoss()
    # optimizer = optim.Adam(model.parameters(), lr=0.01)
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weightdecay, momentum=0.9)
    
    test_data = split_data[iter]
    train_data = np.concatenate(split_data[:iter]+split_data[iter+1:], axis=0)
    test_set = Dataset(test_data)
    train_set = Dataset(train_data)
    train_loader = data.DataLoader(train_set, batch_size=batchsize, shuffle=True, drop_last=False)
    test_loader = data.DataLoader(test_set, batch_size=batchsize, shuffle=True, drop_last=False)

    min_test_loss = 10
    min_train_loss = 10
    min_train_acc = 10
    min_test_acc = 10
    
    for epoch in range(num_epochs):
        # print("START OF EPOCH: " + str(epoch)+ " FOLD: " + str(iter))
        
        train_single_epoch(model, train_loader)
        
        val_loss, val_acc = test_classify(model, test_loader)
        train_loss, train_acc = test_classify(model, train_loader)

        if (epoch == 0):
            print('Fold: {:.1f}\tEpoch: {:.1f}\tTrain Loss: {:.4f}\tTrain Accuracy: {:.4f}\tVal Loss: {:.4f}\tVal Accuracy: {:.4f}'.
            format(iter, epoch, train_loss, train_acc, val_loss, val_acc))

        if (epoch%10 == 9):
            print('Fold: {:.1f}\tEpoch: {:.1f}\tTrain Loss: {:.4f}\tTrain Accuracy: {:.4f}\tVal Loss: {:.4f}\tVal Accuracy: {:.4f}'.
            format(iter, epoch, train_loss, train_acc, val_loss, val_acc))
        
        train_accuracies[(iter, epoch)] = train_acc
        test_accuracies[(iter, epoch)] = val_acc
        train_losses[(iter, epoch)] = train_loss
        test_losses[(iter, epoch)] = val_loss

        
        if (val_loss < min_test_loss):
            min_test_loss = val_loss
            min_train_loss = train_loss
            min_train_acc = train_acc
            min_test_acc = val_acc
    
    print("\n\n")

    avg_train_loss += min_train_loss
    avg_train_acc += min_train_acc
    avg_test_loss += min_test_loss
    avg_test_acc += min_test_acc


    # torch.save({
    #     'epoch': epoch,
    #     'model_state_dict': model.state_dict(),
    #     'optimizer_state_dict': optimizer.state_dict(),
    #     'test_acc': avg_test_acc,
    #     'test_loss': avg_test_loss,
    #     'train_acc': avg_train_acc,
    #     'train_loss': avg_train_loss 
    # }, "models/ai_try1_epoch"+str(epoch)+".tar")

torch.save({
    "train_accuracies" : train_accuracies,
    "test_accuracies" : test_accuracies,
    "train_losses" : train_losses,
    "test_losses" : test_losses
}, "models/try2.tar")
print("\n\n\n")
print(train_accuracies)
print("\n\n\n")
print(test_accuracies)
print("\n\n\n")
print(train_losses)
print("\n\n\n")
print(test_losses)

# print(device)
# model.to(device)
# model.eval()
# test_data = split_data[0]
# test_set = Dataset(test_data)
# test_loader = data.DataLoader(test_set, batch_size=batchsize, shuffle=True, drop_last=False)
# val_loss, val_acc = test_classify(model, test_loader)

# print(val_loss, val_acc)

print(avg_train_loss)
print(avg_train_acc)
print(avg_test_loss)
print(avg_test_acc)

train_accuracies
test_accuracies
train_losses
test_losses


import matplotlib.pyplot as plt 

for i in range(K):
    train_acc_points = []
    test_acc_points = []
    train_loss_points = []
    test_loss_points = []
    x = [j for j in range(150)]
    for epoch in range(150):
        train_acc_points.append(train_accuracies[(i, epoch)])
        test_acc_points.append(test_accuracies[(i, epoch)])
        train_loss_points.append(train_losses[(i, epoch)])
        test_loss_points.append(test_losses[(i, epoch)])
    
    plt.plot(x, train_acc_points, label = "train acc")
    plt.plot(x, test_acc_points, label = "test acc")

    
    plt.xlabel('iteration') 
    plt.title('Accuracies (K='+str(i)+")")
    plt.legend()
    plt.show()
    plt.clf()
    plt.plot(x, train_loss_points, label = "train loss")
    plt.plot(x, test_loss_points, label = "test loss")
    plt.xlabel('iteration')
    plt.title('Losses (K='+str(i)+")")
    plt.legend()
    plt.show()
    plt.clf()

for i in range(K):
    min_test_loss = 10
    min_train_loss = 10
    min_train_acc = 10
    min_test_acc = 10
        
    x = [j for j in range(150)]
    for epoch in range(150):
        val_loss = test_losses[(i, epoch)]
        if (val_loss < min_test_loss):
            min_test_loss = val_loss
            min_train_loss = train_losses[(i, epoch)]
            min_train_acc = train_accuracies[(i, epoch)]
            min_test_acc = test_accuracies[(i, epoch)]
    avg_train_loss += min_train_loss
    avg_train_acc += min_train_acc
    avg_test_loss += min_test_loss
    avg_test_acc += min_test_acc


avg_train_loss /= K
avg_train_acc /= K
avg_test_loss /= K
avg_test_acc /= K

print(avg_train_loss)
print(avg_train_acc)
print(avg_test_loss)
print(avg_test_acc)